{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388d50ae",
   "metadata": {},
   "source": [
    "# <center><b> All of the libaries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63cffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\Documents\\Projects\\AutoStoryGenAndUploader\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json \n",
    "import os\n",
    "import dotenv\n",
    "import asyncio\n",
    "import edge_tts\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52cc37",
   "metadata": {},
   "source": [
    "# <center><b> Important Parameters Ex. dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995a9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "groq_base_url = os.getenv(\"GROQ_BASE_URL\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1ffb4",
   "metadata": {},
   "source": [
    "# <b> <center> ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deedca7",
   "metadata": {},
   "source": [
    "# <center> <B> Phase 1\n",
    "## <center><b> Generating Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7266361",
   "metadata": {},
   "source": [
    "## <b> Script Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689edd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"model\": \"llama-3.3-70b-versatile\",\n",
    "    \"temperature\": 0.6,\n",
    "    \"max_tokens\": 1300,\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a professional short-form educational storyteller for social media.\\n\\n\"\n",
    "\n",
    "                \"Your task:\\n\"\n",
    "                \"1) Generate a curiosity-driven topic.\\n\"\n",
    "                \"2) Write ONE continuous, natural, spoken script.\\n\"\n",
    "                \"3) Divide the full script into 6 sequential narration segments.\\n\"\n",
    "                \"4) Provide structured visual metadata for 6 scene changes.\\n\\n\"\n",
    "\n",
    "                \"Script rules:\\n\"\n",
    "                \"- The script must be a single continuous paragraph.\\n\"\n",
    "                \"- Total length must be between 140 and 170 words.\\n\"\n",
    "                \"- The opening must be a strong scroll-stopping hook.\\n\"\n",
    "                \"- The script must logically progress from start to finish.\\n\"\n",
    "                \"- Maintain engagement through curiosity and clarity.\\n\"\n",
    "                \"- No repetition.\\n\"\n",
    "                \"- No filler.\\n\"\n",
    "                \"- Avoid list-style phrasing.\\n\"\n",
    "                \"- Use clear, direct language.\\n\"\n",
    "                \"- Avoid metaphors and poetic expressions.\\n\"\n",
    "                \"- Avoid dramatic or abstract wording.\\n\"\n",
    "                \"- Write in a conversational documentary tone.\\n\"\n",
    "                \"- The script must sound like a confident YouTube explainer.\\n\"\n",
    "                \"- Use natural punctuation with proper commas and full stops.\\n\"\n",
    "                \"- The script must flow smoothly when spoken aloud.\\n\\n\"\n",
    "\n",
    "                \"Scene narration splitting rules:\\n\"\n",
    "                \"- Split the full_script into exactly 6 sequential narration segments.\\n\"\n",
    "                \"- Each scene narration must be an exact continuous portion of full_script.\\n\"\n",
    "                \"- Do NOT rewrite or paraphrase the text.\\n\"\n",
    "                \"- Do NOT modify wording.\\n\"\n",
    "                \"- The combined scene narrations must exactly reconstruct full_script.\\n\"\n",
    "                \"- Each scene narration should represent a logical progression point.\\n\\n\"\n",
    "\n",
    "                \"Scene metadata rules:\\n\"\n",
    "                \"- Exactly 6 scenes.\\n\"\n",
    "                \"- Each scene must include:\\n\"\n",
    "                \"   • scene_number\\n\"\n",
    "                \"   • narration (exact segment from full_script)\\n\"\n",
    "                \"   • main_subject (specific and concrete)\\n\"\n",
    "                \"   • environment (clear physical setting)\\n\"\n",
    "                \"   • mood (realistic emotional tone)\\n\"\n",
    "                \"   • lighting (clear lighting description)\\n\"\n",
    "                \"   • camera (wide shot, close-up, aerial view, etc.)\\n\"\n",
    "                \"   • key_elements (2 to 4 specific visible elements)\\n\"\n",
    "                \"- Scenes must follow the logical progression of the script.\\n\"\n",
    "                \"- Avoid vague phrases like 'dramatic scene' or 'mysterious setting'.\\n\\n\"\n",
    "\n",
    "                \"Return ONLY valid JSON.\\n\"\n",
    "                \"No explanation outside JSON.\\n\\n\"\n",
    "\n",
    "                \"JSON format:\\n\"\n",
    "                \"{\\n\"\n",
    "                '  \"title\": \"\",\\n'\n",
    "                '  \"full_script\": \"\",\\n'\n",
    "                '  \"scenes\": [\\n'\n",
    "                \"    {\\n\"\n",
    "                '      \"scene_number\": 1,\\n'\n",
    "                '      \"narration\": \"\",\\n'\n",
    "                '      \"main_subject\": \"\",\\n'\n",
    "                '      \"environment\": \"\",\\n'\n",
    "                '      \"mood\": \"\",\\n'\n",
    "                '      \"lighting\": \"\",\\n'\n",
    "                '      \"camera\": \"\",\\n'\n",
    "                '      \"key_elements\": []\\n'\n",
    "                \"    }\\n\"\n",
    "                \"  ]\\n\"\n",
    "                \"}\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Generate a high-retention short-form educational script.\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8049f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "request = requests.post(groq_base_url,headers=headers,data=json.dumps(body))\n",
    "response = request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d00344",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "data = json.loads(raw_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d76022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Human Brain', 'full_script': \"Did you know the human brain uses 20% of our energy? The brain is a complex organ, controlling our movements, it processes information, and enables us to think. It consists of many parts, including the cerebrum, cerebellum, and brainstem. The cerebrum is responsible for our thoughts, the cerebellum for our movements, and the brainstem connects the brain to the spinal cord. Our brain is protected by the skull, and it's made up of billions of neurons. These neurons communicate with each other, enabling us to learn, remember, and adapt to new situations.\", 'scenes': [{'scene_number': 1, 'narration': 'Did you know the human brain uses 20% of our energy?', 'main_subject': 'Human Brain', 'environment': 'Laboratory', 'mood': 'Informative', 'lighting': 'Bright artificial light', 'camera': 'Wide shot', 'key_elements': ['Brain model', 'Energy consumption graph']}, {'scene_number': 2, 'narration': 'The brain is a complex organ, controlling our movements, it processes information, and enables us to think.', 'main_subject': 'Brain Functions', 'environment': 'Brain scanning room', 'mood': 'Curious', 'lighting': 'Soft natural light', 'camera': 'Close-up', 'key_elements': ['Brain scan images', 'Neurologist']}, {'scene_number': 3, 'narration': 'It consists of many parts, including the cerebrum, cerebellum, and brainstem.', 'main_subject': 'Brain Structure', 'environment': 'Anatomy classroom', 'mood': 'Educational', 'lighting': 'Overhead fluorescent lights', 'camera': 'Aerial view', 'key_elements': ['Brain diagrams', 'Anatomy textbooks']}, {'scene_number': 4, 'narration': 'The cerebrum is responsible for our thoughts, the cerebellum for our movements, and the brainstem connects the brain to the spinal cord.', 'main_subject': 'Cerebrum and Cerebellum', 'environment': 'Neurology laboratory', 'mood': 'Fascinating', 'lighting': 'Dim red light', 'camera': 'Macro shot', 'key_elements': ['Cerebrum model', 'Cerebellum diagram']}, {'scene_number': 5, 'narration': \"Our brain is protected by the skull, and it's made up of billions of neurons.\", 'main_subject': 'Neurons and Skull', 'environment': 'Skull exhibit', 'mood': 'Intriguing', 'lighting': 'Softbox lights', 'camera': 'Close-up', 'key_elements': ['Skull model', 'Neuron illustrations']}, {'scene_number': 6, 'narration': 'These neurons communicate with each other, enabling us to learn, remember, and adapt to new situations.', 'main_subject': 'Neural Communication', 'environment': 'Neuroscience research center', 'mood': 'Inspiring', 'lighting': 'Natural daylight', 'camera': 'Wide shot', 'key_elements': ['Neuron connections diagram', 'Researchers']}]}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cddfcd",
   "metadata": {},
   "source": [
    "# <center> <b> --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb2bf3",
   "metadata": {},
   "source": [
    "# <center> <b> Phase 2 \n",
    "# <center> <b> Generating Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bf506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scene_rates = {\n",
    "    1: \"+8%\",\n",
    "    2: \"+5%\",\n",
    "    3: \"+3%\",\n",
    "    4: \"+3%\",\n",
    "    5: \"+2%\",\n",
    "    6: \"+0%\"\n",
    "}\n",
    "\n",
    "async def generate_scene_audio(text, scene_number):\n",
    "    rate = scene_rates.get(scene_number, \"+3%\")\n",
    "    communicate = edge_tts.Communicate(\n",
    "        text,\n",
    "        voice=\"en-US-GuyNeural\",\n",
    "        rate=rate\n",
    "    )\n",
    "    await communicate.save(f\"audio/scene_{scene_number}.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44a36650",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"audio\"):\n",
    "    os.makedirs(\"audio\")\n",
    "async def generate_all_scenes(data):\n",
    "    tasks = []\n",
    "    for scene in data[\"scenes\"]:\n",
    "        tasks.append(\n",
    "            generate_scene_audio(\n",
    "                scene[\"narration\"],\n",
    "                scene[\"scene_number\"]\n",
    "            )\n",
    "        )\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "await generate_all_scenes(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfb7f3e",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------\n",
    "# <center><b>Phase 3\n",
    "# <center><b> Generating Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c2a575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 25.84it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "c:\\Users\\LEGION\\Documents\\Projects\\AutoStoryGenAndUploader\\venv\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:2186: FutureWarning: `enable_vae_slicing` is deprecated and will be removed in version 0.40.0. Calling `enable_vae_slicing()` on a `StableDiffusionPipeline` is deprecated and this method will be removed in a future version. Please use `pipe.vae.enable_slicing()`.\n",
      "  deprecate(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = StableDiffusionPipeline.from_single_file(\n",
    "    \"models/dreamshaper_8.safetensors\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_vae_slicing()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb066520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def genImage(prompt,negative_prompt,output_path):\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=1024,\n",
    "        width=768,\n",
    "        num_inference_steps=28,\n",
    "        guidance_scale=7.5\n",
    "    ).images[0]\n",
    "\n",
    "    image.save(f\"images/scene_{output_path}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacb1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(scene):\n",
    "    return f\"\"\"\n",
    "    cinematic stylized illustration of {scene['main_subject']} in {scene['environment']},\n",
    "    mood: {scene['mood']},\n",
    "    lighting: {scene['lighting']},\n",
    "    {scene['camera']},\n",
    "    featuring {', '.join(scene['key_elements'])},\n",
    "    dramatic lighting,\n",
    "    high contrast,\n",
    "    strong depth perspective,\n",
    "    professional digital artwork\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "negative_prompt = \"\"\"\n",
    "low quality, blurry, flat lighting, oversaturated, cluttered background,\n",
    "extra creatures, extra objects, distorted anatomy, noisy image,\n",
    "grainy texture, cartoonish, low contrast , female character , Women , female\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c49eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    cinematic stylized illustration of Human Brain in Laboratory,\n",
      "    mood: Informative,\n",
      "    lighting: Bright artificial light,\n",
      "    Wide shot,\n",
      "    featuring Brain model, Energy consumption graph,\n",
      "    dramatic lighting,\n",
      "    high contrast,\n",
      "    strong depth perspective,\n",
      "    professional digital artwork\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:21<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    cinematic stylized illustration of Brain Functions in Brain scanning room,\n",
      "    mood: Curious,\n",
      "    lighting: Soft natural light,\n",
      "    Close-up,\n",
      "    featuring Brain scan images, Neurologist,\n",
      "    dramatic lighting,\n",
      "    high contrast,\n",
      "    strong depth perspective,\n",
      "    professional digital artwork\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 20/28 [00:15<00:06,  1.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m prompt = build_prompt(scene)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mgenImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscene\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscene_number\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgenImage\u001b[39m\u001b[34m(prompt, negative_prompt, output_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenImage\u001b[39m(prompt,negative_prompt,output_path):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     image = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7.5\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.images[\u001b[32m0\u001b[39m]\n\u001b[32m     11\u001b[39m     image.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimages/scene_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LEGION\\Documents\\Projects\\AutoStoryGenAndUploader\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LEGION\\Documents\\Projects\\AutoStoryGenAndUploader\\venv\\Lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:1061\u001b[39m, in \u001b[36mStableDiffusionPipeline.__call__\u001b[39m\u001b[34m(self, prompt, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[39m\n\u001b[32m   1058\u001b[39m     noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=\u001b[38;5;28mself\u001b[39m.guidance_rescale)\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1061\u001b[39m latents = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_step_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m callback_on_step_end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1064\u001b[39m     callback_kwargs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LEGION\\Documents\\Projects\\AutoStoryGenAndUploader\\venv\\Lib\\site-packages\\diffusers\\schedulers\\scheduling_pndm.py:256\u001b[39m, in \u001b[36mPNDMScheduler.step\u001b[39m\u001b[34m(self, model_output, timestep, sample, return_dict)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_prk(model_output=model_output, timestep=timestep, sample=sample, return_dict=return_dict)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_plms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LEGION\\Documents\\Projects\\AutoStoryGenAndUploader\\venv\\Lib\\site-packages\\diffusers\\schedulers\\scheduling_pndm.py:379\u001b[39m, in \u001b[36mPNDMScheduler.step_plms\u001b[39m\u001b[34m(self, model_output, timestep, sample, return_dict)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    377\u001b[39m     model_output = (\u001b[32m1\u001b[39m / \u001b[32m24\u001b[39m) * (\u001b[32m55\u001b[39m * \u001b[38;5;28mself\u001b[39m.ets[-\u001b[32m1\u001b[39m] - \u001b[32m59\u001b[39m * \u001b[38;5;28mself\u001b[39m.ets[-\u001b[32m2\u001b[39m] + \u001b[32m37\u001b[39m * \u001b[38;5;28mself\u001b[39m.ets[-\u001b[32m3\u001b[39m] - \u001b[32m9\u001b[39m * \u001b[38;5;28mself\u001b[39m.ets[-\u001b[32m4\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m prev_sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_prev_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_timestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28mself\u001b[39m.counter += \u001b[32m1\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LEGION\\Documents\\Projects\\AutoStoryGenAndUploader\\venv\\Lib\\site-packages\\diffusers\\schedulers\\scheduling_pndm.py:425\u001b[39m, in \u001b[36mPNDMScheduler._get_prev_sample\u001b[39m\u001b[34m(self, sample, timestep, prev_timestep, model_output)\u001b[39m\n\u001b[32m    423\u001b[39m alpha_prod_t = \u001b[38;5;28mself\u001b[39m.alphas_cumprod[timestep]\n\u001b[32m    424\u001b[39m alpha_prod_t_prev = \u001b[38;5;28mself\u001b[39m.alphas_cumprod[prev_timestep] \u001b[38;5;28;01mif\u001b[39;00m prev_timestep >= \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.final_alpha_cumprod\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m beta_prod_t = \u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_prod_t\u001b[49m\n\u001b[32m    426\u001b[39m beta_prod_t_prev = \u001b[32m1\u001b[39m - alpha_prod_t_prev\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.prediction_type == \u001b[33m\"\u001b[39m\u001b[33mv_prediction\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LEGION\\Documents\\Projects\\AutoStoryGenAndUploader\\venv\\Lib\\site-packages\\torch\\_tensor.py:33\u001b[39m, in \u001b[36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m(f):\n\u001b[32m     31\u001b[39m     assigned = functools.WRAPPER_ASSIGNMENTS\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;129m@functools\u001b[39m.wraps(f, assigned=assigned)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m             \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[32m     37\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")\n",
    "for scene in data[\"scenes\"]:\n",
    "    prompt = build_prompt(scene)\n",
    "    print(prompt)\n",
    "    genImage(prompt, negative_prompt, scene[\"scene_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f48ff",
   "metadata": {},
   "source": [
    "# <center> ------------------------------------------------------------\n",
    "# <center><b> Phase 4\n",
    "# <center> <b>Stitching Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7106beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_video.mp4.\n",
      "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_video.mp4\n",
      "Smooth cinematic video created.\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips\n",
    "\n",
    "video_clips = []\n",
    "\n",
    "for scene in data[\"scenes\"]:\n",
    "    scene_number = scene[\"scene_number\"]\n",
    "    \n",
    "    # Correct path formatting\n",
    "    image_path = f\"images/scene_{scene_number}.png\"\n",
    "    audio_path = f\"audio/scene_{scene_number}.mp3\"\n",
    "    \n",
    "    audio = AudioFileClip(audio_path)\n",
    "    \n",
    "    # Set image duration equal to audio duration\n",
    "    image = ImageClip(image_path).set_duration(audio.duration)\n",
    "    \n",
    "    # Subtle cinematic zoom (Ken Burns effect)\n",
    "    image = image.resize(lambda t: 1 + 0.02 * t)\n",
    "    \n",
    "    # Attach audio\n",
    "    image = image.set_audio(audio)\n",
    "    \n",
    "    # Smooth crossfade (skip first clip)\n",
    "    if len(video_clips) > 0:\n",
    "        image = image.crossfadein(0.6)\n",
    "    \n",
    "    video_clips.append(image)\n",
    "\n",
    "# Overlap clips for crossfade\n",
    "final_video = concatenate_videoclips(\n",
    "    video_clips,\n",
    "    method=\"compose\",\n",
    "    padding=-0.6\n",
    ")\n",
    "\n",
    "final_video.write_videofile(\n",
    "    \"final_video.mp4\",\n",
    "    fps=24,\n",
    "    codec=\"libx264\",\n",
    "    audio_codec=\"aac\"\n",
    ")\n",
    "\n",
    "print(\"Smooth cinematic video created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bdd92",
   "metadata": {},
   "source": [
    "# <center> <b> ----------------------------------------\n",
    "# <center><b>Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa78437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_video_muted.mp4.\n",
      "Moviepy - Writing video final_video_muted.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_video_muted.mp4\n",
      "Muted video saved: final_video_muted.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "INPUT_VIDEO = \"final_video.mp4\"\n",
    "OUTPUT_VIDEO = \"final_video_muted.mp4\"\n",
    "\n",
    "# Load video\n",
    "video = VideoFileClip(INPUT_VIDEO)\n",
    "\n",
    "# Remove audio\n",
    "video_no_audio = video.without_audio()\n",
    "\n",
    "# Export muted video\n",
    "video_no_audio.write_videofile(\n",
    "    OUTPUT_VIDEO,\n",
    "    codec=\"libx264\",\n",
    "    audio=False,\n",
    "    fps=24\n",
    ")\n",
    "\n",
    "print(\"Muted video saved:\", OUTPUT_VIDEO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "930babe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_script(text):\n",
    "    text = text.replace(\". \", \", \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fb12f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full narration saved: audio/full_audio.mp3\n",
      "Full narration saved: audio/full_audio.mp3\n"
     ]
    }
   ],
   "source": [
    "import edge_tts\n",
    "import asyncio\n",
    "\n",
    "VOICE = \"en-US-AndrewNeural\"\n",
    "OUTPUT_AUDIO = \"audio/full_audio.mp3\"\n",
    "\n",
    "async def generate_full_audio(full_script):\n",
    "    communicate = edge_tts.Communicate(\n",
    "        text=full_script,\n",
    "        voice=VOICE,\n",
    "        rate=\"+4%\"\n",
    "    )\n",
    "\n",
    "    await communicate.save(OUTPUT_AUDIO)\n",
    "    print(\"Full narration saved:\", OUTPUT_AUDIO)\n",
    "\n",
    "# In Jupyter:\n",
    "cleaned_script = smooth_script(data[\"full_script\"])\n",
    "await generate_full_audio(cleaned_script)\n",
    "await generate_full_audio(data[\"full_script\"])\n",
    "\n",
    "# In script:\n",
    "# asyncio.run(generate_full_audio(data[\"full_script\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18f00435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video Video.mp4.\n",
      "MoviePy - Writing audio in VideoTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video Video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready Video.mp4\n",
      "Final video created: Video.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "VIDEO_PATH = \"final_video_muted.mp4\"\n",
    "AUDIO_PATH = \"audio/full_audio.mp3\"\n",
    "OUTPUT_PATH = \"Video.mp4\"\n",
    "\n",
    "# Load video\n",
    "video = VideoFileClip(VIDEO_PATH)\n",
    "\n",
    "# Load audio\n",
    "audio = AudioFileClip(AUDIO_PATH)\n",
    "\n",
    "# If audio is longer than video → trim audio\n",
    "if audio.duration > video.duration:\n",
    "    audio = audio.subclip(0, video.duration)\n",
    "\n",
    "# If video is longer than audio → trim video\n",
    "if video.duration > audio.duration:\n",
    "    video = video.subclip(0, audio.duration)\n",
    "\n",
    "# Attach audio\n",
    "final_video = video.set_audio(audio)\n",
    "\n",
    "# Export\n",
    "final_video.write_videofile(\n",
    "    OUTPUT_PATH,\n",
    "    codec=\"libx264\",\n",
    "    audio_codec=\"aac\",\n",
    "    fps=video.fps\n",
    ")\n",
    "\n",
    "print(\"Final video created:\", OUTPUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
